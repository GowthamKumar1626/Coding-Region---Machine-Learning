{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readseq(fileName):\n",
    "    try:\n",
    "        seq = ''\n",
    "        with open(fileName, 'r') as f:\n",
    "            for line in f:\n",
    "                seq += line.rstrip()\n",
    "        return seq\n",
    "    except:\n",
    "        print(\"Error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"SeqModel.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['Coding_Region'] = dataset['Coding_Region'].map({'Yes':1,'No':0})\n",
    "encoder = LabelEncoder()\n",
    "dataset['Coding_Region'] = encoder.fit_transform(dataset['Coding_Region'])\n",
    "dataset['Coding_Region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_vals = dataset['Coding_Region'].value_counts()\n",
    "col_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why we need to scale the attributes?\n",
    "#With Out Normalization\n",
    "GCPercent = dataset.iloc[:,2:3].values\n",
    "Length = dataset.iloc[:,1:2].values\n",
    "plt.scatter(Length, GCPercent)\n",
    "plt.show()\n",
    "\n",
    "#After scaling the attributes using standard scaler\n",
    "sc = StandardScaler()\n",
    "GCPercent = sc.fit_transform(GCPercent)\n",
    "Length = sc.fit_transform(Length)\n",
    "plt.scatter(Length, GCPercent)\n",
    "plt.show()\n",
    "\n",
    "#After Min-Max Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(GCPercent)\n",
    "GCPercent = scaler.transform(GCPercent)\n",
    "scaler.fit(Length)\n",
    "Length = scaler.transform(Length)\n",
    "plt.scatter(Length, GCPercent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box Plot of different attributes\n",
    "\n",
    "df = pd.DataFrame(dataset.iloc[:,1:2].values)\n",
    "df.plot.box(grid='True')\n",
    "df = pd.DataFrame(dataset.iloc[:,2:3].values)\n",
    "df.plot.box(grid='True')\n",
    "df = pd.DataFrame(dataset.iloc[:,4:8].values)\n",
    "df.plot.box(grid='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthGC = dataset.iloc[:,1:3].values\n",
    "lengthGC = pd.DataFrame(lengthGC)\n",
    "c_mat = lengthGC.corr()\n",
    "c_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mat = dataset.corr()\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "sb.heatmap(c_mat, vmax = .8, square = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the attribute values from dataset into numpy.ndarray\n",
    "X = dataset.iloc[:,1:24].values\n",
    "Y = dataset.iloc[:,24].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram - Visualizing the normalized data\n",
    "X_hist = X\n",
    "sc = StandardScaler()\n",
    "X_s = sc.fit_transform(X_hist)\n",
    "X_s = pd.DataFrame(X_s)   \n",
    "\n",
    "import scipy.stats as stats\n",
    "param = stats.norm.fit(X_s)\n",
    "x = np.linspace(-2, 8, 4385)\n",
    "pdf_fitted = stats.norm.pdf(x,*param)\n",
    "X_s.plot.hist(alpha=0.5, bins=15, grid=True, legend=None)\n",
    "plt.xlabel(\"Feature value\")\n",
    "plt.title(\"Histogram with fitted Normalized distribution\")\n",
    "plt.plot(x, pdf_fitted, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train and test\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No.of training and testing records\n",
    "print(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Scaling using Standard scaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_Forest = RandomForestClassifier(n_estimators = 20,criterion='entropy')\n",
    "# model_Forest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_Forest.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "model_Logistic = LogisticRegression(random_state = 5)\n",
    "#Training the model\n",
    "model_Logistic.fit(X_train, Y_train)\n",
    "#Score of our model\n",
    "print(\"Accuracy from Logistic regression model:\",model_Logistic.score(X_test, Y_test))\n",
    "#Prediction \n",
    "Predicted_logistic = model_Logistic.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "cf_logistic = confusion_matrix(Y_test,Predicted_logistic)\n",
    "print(\"Confusion matrix:\\n\",cf_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "\n",
    "model_decision = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "#Training the model\n",
    "model_decision.fit(X_train, Y_train)\n",
    "#Score of our model\n",
    "print(\"Accuracy from Decision tree model:\",model_decision.score(X_test, Y_test))\n",
    "#Prediction \n",
    "Predicted_decision = model_decision.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "cf_desicion = confusion_matrix(Y_test,Predicted_decision)\n",
    "print(\"Confusion matrix:\\n\",cf_desicion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "model_Forest = RandomForestClassifier(n_estimators = 20,criterion='entropy')\n",
    "#Training the model\n",
    "model_Forest.fit(X_train, Y_train)\n",
    "#Score of our model\n",
    "print(\"Accuracy from Random Forest model:\",model_Forest.score(X_test, Y_test))\n",
    "#Prediction \n",
    "Predicted_forest = model_Forest.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "cf_forest = confusion_matrix(Y_test,Predicted_forest)\n",
    "print(\"Confusion matrix:\\n\",cf_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive-Bayes\n",
    "\n",
    "model_naive = GaussianNB()\n",
    "#Training the model\n",
    "model_naive.fit(X_train, Y_train)\n",
    "#Score of our model\n",
    "print(\"Accuracy from Naive-Bayes model:\",model_naive.score(X_test, Y_test))\n",
    "#Prediction \n",
    "Predicted_naive = model_naive.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "cf_naive = confusion_matrix(Y_test,Predicted_naive)\n",
    "print(\"Confusion matrix:\\n\",cf_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "\n",
    "model_SVC = SVC(kernel = \"linear\", random_state = 10)\n",
    "#Training the model\n",
    "model_SVC.fit(X_train, Y_train)\n",
    "#Score of our model\n",
    "print(\"Accuracy from SVC model:\",model_SVC.score(X_test, Y_test))\n",
    "#Prediction \n",
    "Predicted_SVC = model_SVC.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "cf_SVC = confusion_matrix(Y_test,Predicted_SVC)\n",
    "print(\"Confusion matrix:\\n\",cf_SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "\n",
    "model_xgboost = XGBClassifier()\n",
    "#Training the model\n",
    "model_xgboost.fit(X_train, Y_train)\n",
    "#Score of our model\n",
    "print(\"Accuracy from XGBoost model:\",model_xgboost.score(X_test, Y_test))\n",
    "#Prediction \n",
    "Predicted_xgboost = model_xgboost.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "cf_xgboost = confusion_matrix(Y_test,Predicted_xgboost)\n",
    "print(\"Confusion matrix:\\n\",cf_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CatBoost\n",
    "\n",
    "model_catboost = CatBoostClassifier(iterations=10, learning_rate=1,depth=2)\n",
    "#Training the model\n",
    "model_catboost.fit(X_train, Y_train)\n",
    "#Score of our model\n",
    "print(\"Accuracy from CatBoost model:\",model_catboost.score(X_test, Y_test))\n",
    "#Prediction \n",
    "Predicted_catboost = model_catboost.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "cf_catboost = confusion_matrix(Y_test,Predicted_catboost)\n",
    "print(\"Confusion matrix:\\n\",cf_catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Artificial Neural Network\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(activation=\"relu\", input_dim=23, units=6, kernel_initializer=\"uniform\"))\n",
    "classifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n",
    "classifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "classifier.fit(X_train, Y_train, batch_size = 10, epochs = 100,validation_split = 0.2)\n",
    "\n",
    "#Prediction\n",
    "ann_Predict = classifier.predict(X_test)\n",
    "\n",
    "#Prediction Analysis\n",
    "plt.plot(Y_test, color = 'red', label = 'Real data')\n",
    "plt.plot(ann_Predict, color = 'blue', label = 'Predicted data')\n",
    "plt.title('Prediction Probability Ranges')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation for logistic regression\n",
    "accuracy = cross_val_score(estimator = model_Logistic, X = X_train, y = Y_train, cv = 10, n_jobs = -1)\n",
    "print(\"Mean of 10 accuracies:\",accuracy.mean())\n",
    "print(\"Standard Deviation of predicted accuracies:\",accuracy.std())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation for Decision tree\n",
    "accuracy = cross_val_score(estimator = model_decision, X = X_train, y = Y_train, cv = 10, n_jobs = -1)\n",
    "print(\"Mean of 10 accuracies:\",accuracy.mean())\n",
    "print(\"Standard Deviation of predicted accuracies:\",accuracy.std())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation for Random Forest\n",
    "accuracy = cross_val_score(estimator = model_Forest, X = X_train, y = Y_train, cv = 10, n_jobs = -1)\n",
    "print(\"Mean of 10 accuracies:\",accuracy.mean())\n",
    "print(\"Standard Deviation of predicted accuracies:\",accuracy.std())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation for Naive-Bayes\n",
    "accuracy = cross_val_score(estimator = model_naive, X = X_train, y = Y_train, cv = 10, n_jobs = -1)\n",
    "print(\"Mean of 10 accuracies:\",accuracy.mean())\n",
    "print(\"Standard Deviation of predicted accuracies:\",accuracy.std())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation for SVC\n",
    "accuracy = cross_val_score(estimator = model_SVC, X = X_train, y = Y_train, cv = 10, n_jobs = -1)\n",
    "print(\"Mean of 10 accuracies:\",accuracy.mean())\n",
    "print(\"Standard Deviation of predicted accuracies:\",accuracy.std())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation for XGBOOST\n",
    "accuracy = cross_val_score(estimator = model_xgboost, X = X_train, y = Y_train, cv = 10, n_jobs = -1)\n",
    "print(\"Mean of 10 accuracies:\",accuracy.mean())\n",
    "print(\"Standard Deviation of predicted accuracies:\",accuracy.std())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation for CAT BOOST\n",
    "accuracy = cross_val_score(estimator = model_catboost, X = X_train, y = Y_train, cv = 10, n_jobs = -1)\n",
    "print(\"Mean of 10 accuracies:\",accuracy.mean())\n",
    "print(\"Standard Deviation of predicted accuracies:\",accuracy.std())\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve for XGBoost\n",
    "fpr2, tpr2, _ = roc_curve(Y_test,\n",
    "                          Predicted_xgboost,\n",
    "                          pos_label = 1)\n",
    "auc_rf = auc(fpr2, tpr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, auc, estimator, xlim=None, ylim=None):\n",
    "    my_estimators = {'knn': ['Kth Nearest Neighbor', 'deeppink'],\n",
    "              'rf': ['XGBoost', 'red'],\n",
    "              'nn': ['Neural Network', 'purple']}\n",
    "\n",
    "    try:\n",
    "        plot_title = my_estimators[estimator][0]\n",
    "        color_value = my_estimators[estimator][1]\n",
    "    except KeyError as e:\n",
    "        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n",
    "\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n",
    "        raise\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.set_facecolor('#fafafa')\n",
    "\n",
    "    plt.plot(fpr, tpr,\n",
    "             color=color_value,\n",
    "             linewidth=1)\n",
    "    plt.title('ROC Curve For {0} (AUC = {1: 0.3f})'\\\n",
    "              .format(plot_title, auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.plot([0, 0], [1, 0], 'k--', lw=2, color = 'black')\n",
    "    plt.plot([1, 0], [1, 1], 'k--', lw=2, color = 'black')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(*xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr2, tpr2, auc_rf, 'rf',\n",
    "               xlim=(-0.01, 1.05), \n",
    "               ylim=(0.001, 1.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr2, tpr2, auc_rf, 'rf', \n",
    "               xlim=(-0.01, 0.2), \n",
    "               ylim=(0.85, 1.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Predicting a sequence will be in coding or non-coding sequence\n",
    "\n",
    "def prediction_XGBoost(seq):\n",
    "    try:\n",
    "        pred = []\n",
    "\n",
    "        #Length\n",
    "        pred.append(len(seq))\n",
    "\n",
    "        #GC Count\n",
    "        count = 0\n",
    "        for j in range(len(seq)):\n",
    "            if(seq[j:j+2] == 'GC'):\n",
    "                count += 1\n",
    "        GC = (count/len(seq))*100\n",
    "        GC = round(GC,3)\n",
    "        pred.append(GC)\n",
    "\n",
    "        #ATG Presence\n",
    "        if 'ATG' in seq:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "\n",
    "        #Single Nucleotide Probability\n",
    "        nucleotides = {'A':0,'C':0,'G':0,'T':0}\n",
    "        prob = {'A':0,'C':0,'G':0,'T':0}\n",
    "        for base in seq:\n",
    "            nucleotides[base] += 1\n",
    "        for i in prob:\n",
    "            prob[i] = round(nucleotides[i]/len(seq),3) \n",
    "        for i in prob:\n",
    "            pred.append(prob[i])\n",
    "\n",
    "\n",
    "        #DiNucleotide Probability\n",
    "        nucleotideProbability = {'AA':0,'AC':0,'AG':0,'AT':0,'TA':0,'TC':0,'TG':0,'TT':0,'CA':0,'CC':0,'CG':0,'CT':0,'GA':0,'GC':0,'GG':0,'GT':0}\n",
    "        for i in range(len(seq)):\n",
    "            if len(seq[i:i+2:1]) == 2:\n",
    "                nucleotideProbability[seq[i:i+2:1]] += 1\n",
    "        for i in nucleotideProbability:\n",
    "            nucleotideProbability[i] = round(nucleotideProbability[i]/len(seq),3)\n",
    "\n",
    "\n",
    "        #Conditional Probability\n",
    "        condiProb = {'AA':0,'AC':0,'AG':0,'AT':0,'TA':0,'TC':0,'TG':0,'TT':0,'CA':0,'CC':0,'CG':0,'CT':0,'GA':0,'GC':0,'GG':0,'GT':0}\n",
    "        for i in condiProb:\n",
    "            if prob[i[0:1]] == 0:\n",
    "                condiProb[i] = 0\n",
    "            else:\n",
    "                condiProb[i] = round(nucleotideProbability[i]/prob[i[0:1]],3)\n",
    "        for i in condiProb:\n",
    "            pred.append(condiProb[i])\n",
    "\n",
    "\n",
    "        pred = np.array([pred])\n",
    "        pred = pred.reshape(-1, 1)\n",
    "    #     pred = sc.fit_transform(pred)\n",
    "        pred = pred.flatten()\n",
    "        final = np.array([pred])\n",
    "        print(final)\n",
    "\n",
    "        #pred = np.arange(pred).reshape(-1,1)\n",
    "        predict_XGBoost = model_xgboost.predict(final)\n",
    "        print(predict_XGBoost)\n",
    "        return predict_XGBoost\n",
    "    except:\n",
    "        print(\"SEQUENCE WRONG!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = readseq('Seq.txt')\n",
    "predict = prediction_XGBoost(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Fitting K-Means to the dataset\n",
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n",
    "plt.title('Clusters of Proteins')\n",
    "plt.xlabel('Independent Attributes')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,2:3].values\n",
    "\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Fitting K-Means to the dataset\n",
    "kmeans = KMeans(n_clusters = 6, init = 'k-means++', random_state = 42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[y_kmeans == 0], X[y_kmeans == 0], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1], X[y_kmeans == 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2], X[y_kmeans == 2], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3], X[y_kmeans == 3], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_kmeans == 4], X[y_kmeans == 4], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "plt.scatter(X[y_kmeans == 5], X[y_kmeans == 5], s = 100, c = 'black', label = 'Cluster 6')\n",
    "plt.scatter(kmeans.cluster_centers_[:], kmeans.cluster_centers_[:], s = 50, c = 'yellow', label = 'Centroids')\n",
    "plt.title('Clusters of Proteins')\n",
    "plt.xlabel('Independent Attributes')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
